{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from tqdm import tqdm\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/alexeydemyanchuk/Kaggle/DogBreedIdent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataframes for training and testing\n",
    "labels = pd.read_csv('labels.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 1987\n",
    "INPUT_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10222 10222\n",
      "10357 10357\n"
     ]
    }
   ],
   "source": [
    "# print(len(os.listdir(os.path.join(data_dir, 'train'))), len(labels))\n",
    "# print(len(os.listdir(os.path.join(data_dir, 'test'))), len(sample_submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train/valid split indecise\n",
    "np.random.seed(seed=SEED)\n",
    "rnd = np.random.random(len(labels))\n",
    "train_idx = rnd < 0.8\n",
    "valid_idx = rnd >= 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from categorical to one hot \n",
    "selected_breed_list = list(labels.groupby('breed').count().sort_values(by='id', ascending=False).head(120).index)\n",
    "labels['target'] = 1\n",
    "labels['rank'] = labels.groupby('breed').rank()['id']\n",
    "labels_pivot = labels.pivot('id', 'breed', 'target').reset_index().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = labels_pivot[selected_breed_list].values\n",
    "ytr = y_train[train_idx]\n",
    "yv = y_train[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8211, 120) (2011, 120)\n"
     ]
    }
   ],
   "source": [
    "print(ytr.shape, yv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper to read, resize image and convert it to numpy array \n",
    "def read_img(img_id, train_or_test, size):\n",
    "    \"\"\"Read and resize image.\n",
    "    # Arguments\n",
    "        img_id: string\n",
    "        train_or_test: string 'train' or 'test'.\n",
    "        size: resize the original image.\n",
    "    # Returns\n",
    "        Image as numpy array.\n",
    "    \"\"\"\n",
    "    img = image.load_img(os.path.join(data_dir, train_or_test, '%s.jpg' % img_id), target_size=size)\n",
    "    img = image.img_to_array(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10222it [00:48, 211.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images shape: (10222, 64, 64, 3) size: 125,607,936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# process all training images\n",
    "x_train = np.zeros((len(labels), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "for i, img_id in tqdm(enumerate(labels['id'])):\n",
    "    img = read_img(img_id, 'train', (INPUT_SIZE, INPUT_SIZE))\n",
    "    x = preprocess_input(np.expand_dims(img.copy(), axis=0))\n",
    "    x_train[i] = x\n",
    "print('Train Images shape: {} size: {:,}'.format(x_train.shape, x_train.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8211, 64, 64, 3) (2011, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# split training data in training and validation sets\n",
    "Xtr = x_train[train_idx]\n",
    "Xv = x_train[valid_idx]\n",
    "print(Xtr.shape, Xv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# importing pretrained model not including FC layers for imagenet classification\n",
    "model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(INPUT_SIZE, INPUT_SIZE, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# froze layers from pretrained\n",
    "for layer in model.layers[:19]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connecting custom layers to classify dog breeds\n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(ytr.shape[1], activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final = Model(inputs=model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               123000    \n",
      "=================================================================\n",
      "Total params: 16,935,864\n",
      "Trainable params: 2,221,176\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(), ReduceLROnPlateau()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8211 samples, validate on 2011 samples\n",
      "Epoch 1/50\n",
      "8211/8211 [==============================] - 311s - loss: 15.2750 - acc: 0.0208 - val_loss: 14.9360 - val_acc: 0.0323\n",
      "Epoch 2/50\n",
      "8211/8211 [==============================] - 309s - loss: 14.2790 - acc: 0.0697 - val_loss: 14.5038 - val_acc: 0.0497\n",
      "Epoch 3/50\n",
      "8211/8211 [==============================] - 320s - loss: 13.5777 - acc: 0.1118 - val_loss: 14.3441 - val_acc: 0.0567\n",
      "Epoch 4/50\n",
      "8211/8211 [==============================] - 496s - loss: 13.0224 - acc: 0.1477 - val_loss: 14.0580 - val_acc: 0.0681\n",
      "Epoch 5/50\n",
      "8211/8211 [==============================] - 566s - loss: 12.5704 - acc: 0.1777 - val_loss: 13.9526 - val_acc: 0.0696\n",
      "Epoch 6/50\n",
      "8211/8211 [==============================] - 558s - loss: 12.1944 - acc: 0.1994 - val_loss: 13.7620 - val_acc: 0.0781\n",
      "Epoch 7/50\n",
      "8211/8211 [==============================] - 582s - loss: 11.8645 - acc: 0.2268 - val_loss: 13.6090 - val_acc: 0.0835\n",
      "Epoch 8/50\n",
      "8211/8211 [==============================] - 574s - loss: 11.5533 - acc: 0.2484 - val_loss: 13.5828 - val_acc: 0.0845\n",
      "Epoch 9/50\n",
      "8211/8211 [==============================] - 529s - loss: 11.2784 - acc: 0.2700 - val_loss: 13.4311 - val_acc: 0.0875\n",
      "Epoch 10/50\n",
      "8211/8211 [==============================] - 311s - loss: 11.0695 - acc: 0.2843 - val_loss: 13.3799 - val_acc: 0.0885\n",
      "Epoch 11/50\n",
      "8211/8211 [==============================] - 314s - loss: 10.8704 - acc: 0.3002 - val_loss: 13.3163 - val_acc: 0.0895\n",
      "Epoch 12/50\n",
      "8211/8211 [==============================] - 310s - loss: 10.6837 - acc: 0.3162 - val_loss: 13.2803 - val_acc: 0.0885\n",
      "Epoch 13/50\n",
      "8211/8211 [==============================] - 303s - loss: 10.5370 - acc: 0.3237 - val_loss: 13.3159 - val_acc: 0.0900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13e0bec50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.fit(Xtr, ytr, validation_data=(Xv, yv), batch_size=64,\n",
    "                epochs=50, callbacks=callbacks,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(data_dir, \"vgg16_imgnet_pretrained_50epochs.h5\")\n",
    "model_final.save_weights(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
